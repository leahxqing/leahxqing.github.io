---
title: '计算社会科学导论'
date: 2023-09-24
permalink: /posts/mostly_harmless_01/
tags:
  - Machine Learning
  - Python
---

Reading Note for *计算数据科学导论*.

# Parametric Regression Analysis

## Linear Regression

- Overfitting & Underfitting
    - 交叉验证判断模型是否过拟合
        - Hold-out
        - K-fold
    - 缓解过拟合问题
        - 增加观测对象的个数
        - Feature Selection
        - 降维
            - e.g.: Principal Component Analysis (PCA)：将p个预测变量投影到M维的空间内，且M<p
        - 正则化(regularization)
            - 岭回归
            - 拉索回归
- OLS
    - loss function
        
        $$
        RSS=\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2
        $$
        
    - 矩阵形式：
    - loss function (the object function to minimize)
        
        $$
        \hat{\beta}=\mathbf{argmin}_{\beta}||\mathbf{X}\beta-\mathbf{y}||_2^2
        $$
        
    - 解析解：
        
        $$
        \hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
        $$
        

## 岭回归

- loss function
    
    $$
    \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2+\lambda\sum_{j=1}^p\beta_j^2=RSS+\lambda\sum_{j=1}^p\beta_j^2
    $$
    
- 矩阵形式：
    
    $$
    \hat{\beta}=\mathbf{argmin}_{\beta}||\mathbf{X}\beta-\mathbf{y}||_2^2+||\beta^p||_2^2
    $$
    
- $\beta^p$代表去掉偏置后的$p$维向量
    - 增加了一项Shrinkage Penalty$(\lambda\sum_{j=1}^p\beta_j^2)$, $\lambda$ is the hyperparameter
    - the square root of shrinkage penalty is L2 norm
- 岭回归主要克服了OLS中的多重共线问题（完全相关，高度相关）
    - 高度相关：$\mathbb{X}$中各特征向量高度相关，Variance Inflation Factor, VIF很高
    - 解析解
        
        $$
        \hat{\beta}=(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}^p)^{-1}\mathbf{X}^T\mathbf{y}
        $$
        
- Parameter Tuning — $\lambda$
    - Ridge Trace岭迹图
- Example: Residents’ Welfare
    
    ```python
    import pandas as pd
    import numpy as np
    # read csv files x, y
    # skelearn model: train_test_split
    from skelearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=.3. random_state=728)
    ```
## LASSO

- Loss Function

$$
\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2+\lambda\sum_{j=1}^p|\beta_j|=RSS+\lambda\sum_{j=1}^p|\beta_j|
$$

- Matrix form

$$
\hat{\beta}=\mathbf{argmin}_{\beta}||\mathbf{X\beta-y}||_2^2+||\beta^p||_2^2
$$

- $\lambda\sum_{j=1}^p|\beta_j|$: L1 norm
- lasso也会使得系数向0收缩，且$\lambda$越大，收缩越明显。区别在于，岭回归的系数不会等于0，lasso可能等于0 — 会得到一个Sparse Model
  
## 弹力网

- Loss Function

$$
RSS+\lambda((1-\alpha)\sum_{j=1}^p\beta_j^2+\alpha\sum_{j=1}^p|\beta_j|)
$$

- Matrix Form

$$
\hat{\beta}=\mathbf{argmin}_{\beta}||\mathbf{X\beta-y}||_2^2+\lambda(1-\alpha)||\beta^p||_2^2+\lambda\alpha||\beta^p||_2^2
$$

- $\alpha=0:$岭回归，$\alpha=1:$ lasso

# Nonparametric Approach

## K Nearest Neighbors (KNN)

- 基于K个最近邻居的响应变量y来对测试集进行预测
- 表达式：在特征空间中考虑x最近的K个邻居，KNN是以离x最近的K个邻居的y的平均作为预测值。$N_K(x)$为最靠近x的K个观测值$x_i$的集合

$$
\hat{f}_{KNN}(x)=\frac{1}{K}\sum_{x_i\in N_K(x)}y_i
$$

- 距离函数：通常使用欧氏距离法（要求特征变量是数值型）

## Decision Tree

- Basic element
    - Node
        - Root Node, Internal Node, Leaf Node, Terminal Node
    - Branch
- Node Impurity Function
    - CART算法 — Gini Index
    - Information Entropy


## Ensemble Learning

- 主要类别
    - 个体学习器间不存在强依赖关系，可同时生成的并行化方法
    - 个体学习器之间存在强依赖关系，必须串行生成的序列化方法
- EX：基于决策树CART算法的并行式集成学习方法
    - Bootstrap Aggregating (Bagging)
        - 基本步骤
            1. 对训练数据进行有放回的再抽样，得到B个数量的Bootstrapping Sample
            2. 根据每个Bootstrapping Sample训练B棵未修枝的决策树
            3. 对于回归树，将B棵决策树的预测结果进行简单算术平均；对于分类树，将B棵决策树的预测结果进行多数投票，即取众数
    - Random Forest
        - 采用多个决策树训练模型，在袋装法的基础上，从数据集中随机有放回的抽样，再用每个字样本训练决策树
        - Difference compared to Bootstrapping: 在每个决策树的节点分裂时，仅随机选取部分分裂变量作为候选的分类变量，不会考虑其他变量，而在下一点分裂时，在随机选择部分分裂变量作为候选。
            - 分类树：建议$m=\sqrt{p}$, p是全部的特征变量的个数
            - 回归树：$m=p/3$
        - 基本步骤
            1. 预设模型的超参数,比如有几棵树,每棵树的深度
            2. N个数据中随机有放回的抽取n个数据,训练决策树,每个分裂节点从p个特征变量中随机选择m个变量作为候选变量,每次分裂的选取不完全相同,每次抽取的n个数据都训练一棵决策树
            3. 输入待预测样本到每个决策树中,以一定方法将各个树的结果进行整合:通常回归问题求均值,分类问题求众数。
        - m=p: bootstrapping
    - Boosting
        - Sequential Ensemble Approach
        - Adaptive Boosting
        - Also: Gradient Boosting, Stochastic Gradient Boosting, XGBoost

# Cluster Analysis

## Distance

- Minkowski Distance (机器学习中常用)
- Euclidean distance, when Minkowski Distance: p=2
- Manhattan distance, when Minkowski Distance: p=1
- Chebyshev distance: when Minkowski Distance: p = $\infty$

## Prototype-based Clustering

- K-means
- Gaussian Mixture Model, GMM
- Spectral Clustring

## Density-based Clustering

## Hierarchical CLustering

