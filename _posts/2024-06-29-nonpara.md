%# Nonparametric Approach

%## K Nearest Neighbors (KNN)

- 基于K个最近邻居的响应变量y来对测试集进行预测
- 表达式：在特征空间中考虑x最近的K个邻居，KNN是以离x最近的K个邻居的y的平均作为预测值。$$ N_K(x) $$为最靠近x的K个观测值$$ x_i $$的集合

$$
\hat{f}_{KNN}(x)=\frac{1}{K}\sum_{x_i\in N_K(x)}y_i
$$

- 距离函数：通常使用欧氏距离法（要求特征变量是数值型）

## Decision Tree

- Basic element
    - Node
        - Root Node, Internal Node, Leaf Node, Terminal Node
    - Branch
- Node Impurity Function
    - CART算法 — Gini Index
    - Information Entropy


## Ensemble Learning

- 主要类别
    - 个体学习器间不存在强依赖关系，可同时生成的并行化方法
    - 个体学习器之间存在强依赖关系，必须串行生成的序列化方法
- EX：基于决策树CART算法的并行式集成学习方法
    - Bootstrap Aggregating (Bagging)
        - 基本步骤
            1. 对训练数据进行有放回的再抽样，得到B个数量的Bootstrapping Sample
            2. 根据每个Bootstrapping Sample训练B棵未修枝的决策树
            3. 对于回归树，将B棵决策树的预测结果进行简单算术平均；对于分类树，将B棵决策树的预测结果进行多数投票，即取众数
    - Random Forest
        - 采用多个决策树训练模型，在袋装法的基础上，从数据集中随机有放回的抽样，再用每个字样本训练决策树
        - Difference compared to Bootstrapping: 在每个决策树的节点分裂时，仅随机选取部分分裂变量作为候选的分类变量，不会考虑其他变量，而在下一点分裂时，在随机选择部分分裂变量作为候选。
            - 分类树：建议$$ m=\sqrt{p} $$, p是全部的特征变量的个数
            - 回归树：$$ m=p/3 $$
        - 基本步骤
            1. 预设模型的超参数,比如有几棵树,每棵树的深度
            2. N个数据中随机有放回的抽取n个数据,训练决策树,每个分裂节点从p个特征变量中随机选择m个变量作为候选变量,每次分裂的选取不完全相同,每次抽取的n个数据都训练一棵决策树
            3. 输入待预测样本到每个决策树中,以一定方法将各个树的结果进行整合:通常回归问题求均值,分类问题求众数。
        - m=p: bootstrapping
    - Boosting
        - Sequential Ensemble Approach
        - Adaptive Boosting
        - Also: Gradient Boosting, Stochastic Gradient Boosting, XGBoost

# Cluster Analysis

## Distance

- Minkowski Distance (机器学习中常用)
- Euclidean distance, when Minkowski Distance: p=2
- Manhattan distance, when Minkowski Distance: p=1
- Chebyshev distance: when Minkowski Distance: p = $$ \infty $$

## Prototype-based Clustering

- K-means
- Gaussian Mixture Model, GMM
- Spectral Clustring

## Density-based Clustering

## Hierarchical CLustering
